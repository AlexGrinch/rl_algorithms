{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "from atari_wrappers import wrap_deepmind\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistributionalQNetwork:\n",
    "    \n",
    "    def __init__(self, num_actions, state_shape=[84, 84, 4],\n",
    "                 convs=[[32, 8, 4], [64, 4, 2], [64, 3, 1]], \n",
    "                 fully_connected=[512], num_atoms=21, v=(-10, 10),\n",
    "                 optimizer = tf.train.AdamOptimizer(2.5e-4),\n",
    "                 scope=\"q_network\", reuse=False):\n",
    "        \"\"\"Class for neural network which estimates Q-function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        state_shape: list\n",
    "            list of 3 parameters [frame_w, frame_h, num_frames]\n",
    "            frame_w: frame width\n",
    "            frame_h: frame height\n",
    "            num_frames: number of successive frames considered as a state\n",
    "        conv: list\n",
    "            list of convolutional layers' parameters, each element\n",
    "            has the form -- [num_outputs, kernel_size, stride]\n",
    "        fully_connected: list\n",
    "            list of fully connected layers' parameters, each element\n",
    "            has the form -- num_outputs \n",
    "        num_atoms: int\n",
    "            number of atoms in distribution support\n",
    "        v: tuple\n",
    "            tuple of 2 parameters (v_min, v_max)\n",
    "            v_min: minimum q-function value\n",
    "            v_max: maximum q-function value    \n",
    "        optimizer: tf.train optimizer\n",
    "            optimization algorithm for stochastic gradient descend\n",
    "        scope: str\n",
    "            unique name of a specific network\n",
    "        \"\"\"\n",
    "        \n",
    "        xavier = layers.xavier_initializer()\n",
    "        \n",
    "        ###################### Neural network architecture ######################\n",
    "        \n",
    "        input_shape = [None] + state_shape\n",
    "        self.input_states = tf.placeholder(dtype=tf.float32, shape=input_shape)\n",
    "        \n",
    "        # discrete distribution parameters\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min, self.v_max = v\n",
    "        self.delta_z = (self.v_max - self.v_min) / (self.num_atoms - 1)\n",
    "        self.z = [self.v_min + i * self.delta_z for i in range(self.num_atoms)]   \n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # convolutional part of the network\n",
    "            conv_layers = [self.input_states]\n",
    "            with tf.variable_scope(\"conv\"):\n",
    "                for num_outputs, kernel_size, stride in convs:\n",
    "                    conv = layers.convolution2d(conv_layers[-1], \n",
    "                                                num_outputs=num_outputs,\n",
    "                                                kernel_size = kernel_size,\n",
    "                                                stride=stride,\n",
    "                                                padding='VALID',\n",
    "                                                biases_initializer=None,\n",
    "                                                activation_fn=tf.nn.relu)\n",
    "                    conv_layers.append(conv)\n",
    "                self.conv_layers = conv_layers[1:]\n",
    "            self.conv_out = layers.flatten(self.conv_layers[-1])\n",
    "\n",
    "            # fully connected part of the network\n",
    "            fc_layers = [self.conv_out]\n",
    "            with tf.variable_scope(\"fc\"):\n",
    "                for num_outputs in fully_connected:\n",
    "                    fc = layers.fully_connected(fc_layers[-1],\n",
    "                                                num_outputs=num_outputs,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                biases_initializer=None,\n",
    "                                                weights_initializer=xavier)\n",
    "                    fc_layers.append(fc)\n",
    "                self.fc_layers = fc_layers[1:]\n",
    "            self.fc_out = self.fc_layers[-1]\n",
    "            \n",
    "            # probabilities of atoms\n",
    "            self.action_probs = []\n",
    "            with tf.variable_scope(\"actions\"):\n",
    "                for action in range(num_actions):\n",
    "                    action_layer = layers.fully_connected(self.fc_out,\n",
    "                                                          num_outputs=self.num_atoms,\n",
    "                                                          activation_fn=None,\n",
    "                                                          biases_initializer=None,\n",
    "                                                          weights_initializer=xavier)\n",
    "                    action_prob = tf.nn.softmax(action_layer)\n",
    "                    self.action_probs.append(action_prob)\n",
    "                self.state_probs = tf.stack(self.action_probs, axis=1)\n",
    "\n",
    "            # q-values as expectations\n",
    "            with tf.variable_scope(\"q_values\"):\n",
    "                q_values = []\n",
    "                for action in range(num_actions):\n",
    "                    q_value = tf.reduce_sum(self.z * self.action_probs[action])\n",
    "                    q_value = tf.reshape(q_value, [-1, 1])\n",
    "                    q_values.append(q_value)\n",
    "                self.q_state_values = tf.concat(q_values, axis=1)\n",
    "\n",
    "                                                               \n",
    "        ######################### Optimization procedure ########################\n",
    "        \n",
    "        # Q-function approximation\n",
    "        self.input_actions = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "        actions_onehot = tf.one_hot(self.input_actions, num_actions, dtype=tf.float32)\n",
    "        \n",
    "        q_values_selected = tf.multiply(self.q_state_values, actions_onehot)\n",
    "        self.q_action_values = tf.reduce_sum(q_values_selected, axis=1)\n",
    "        self.q_max_values = tf.argmax(self.q_state_values, axis=1)\n",
    "        \n",
    "        actions_onehot_= tf.reshape(actions_onehot, [-1, num_actions, 1])\n",
    "        self.probs_selected = tf.reduce_sum(tf.multiply(self.state_probs, actions_onehot_), axis=1)\n",
    "  \n",
    "        self.input_m = tf.placeholder(dtype=tf.float32, shape=[None, self.num_atoms])\n",
    "        \n",
    "        # construct loss function and optimizer\n",
    "        \n",
    "        self.loss = -tf.reduce_sum(self.input_m * tf.log(self.probs_selected))\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.update_model = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    \n",
    "    def cat_proj(self, sess, rewards, states_, actions_, end, gamma=0.99):\n",
    "        \n",
    "        feed_dict = {self.input_states:states_, self.input_actions:actions_}\n",
    "        probs = sess.run(self.probs_selected, feed_dict=feed_dict)\n",
    "        m = np.zeros_like(probs)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        end = np.array(end, dtype=np.float32)\n",
    "        batch_size = rewards.size\n",
    "        \n",
    "        for j in range(self.num_atoms):\n",
    "\n",
    "            Tz = rewards + gamma * end * self.z[j]\n",
    "            Tz = np.minimum(self.v_max, np.maximum(self.v_min, Tz))\n",
    "            b = (Tz - self.v_min) / self.delta_z\n",
    "            l = np.floor(b)\n",
    "            u = np.ceil(b)\n",
    "            \n",
    "            m[np.arange(batch_size), l.astype(int)] += probs[:,j] * (u - b)\n",
    "            m[np.arange(batch_size), u.astype(int)] += probs[:,j] * (b - l) \n",
    "        return m \n",
    "            \n",
    "    \n",
    "    def argmax_q(self, sess, states):\n",
    "        feed_dict = {self.input_states:states}\n",
    "        q_max_values = sess.run(self.q_max_values, feed_dict)\n",
    "        return q_max_values\n",
    "\n",
    "    def q_values(self, sess, states):\n",
    "        feed_dict = {self.input_states:states}\n",
    "        q_state_values = sess.run(self.q_state_values, feed_dict)\n",
    "        return q_state_values\n",
    "\n",
    "    def update(self, sess, states, actions, input_m):\n",
    "        \n",
    "        feed_dict = {self.input_states:states,\n",
    "                     self.input_actions:actions,\n",
    "                     self.input_m:input_m}\n",
    "        \n",
    "        sess.run(self.update_model, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience replay class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.transition = namedtuple('Transition', \n",
    "                                     ('s', 'a', 'r', 's_', 'end'))\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = [*args]\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        batch = np.reshape(batch, [batch_size, 5])\n",
    "        s = np.stack(batch[:,0])\n",
    "        a = batch[:,1]\n",
    "        r = batch[:,2]\n",
    "        s_ = np.stack(batch[:,3])\n",
    "        end = 1 - batch[:,4]\n",
    "        return self.transition(s, a, r, s_, end)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent for playing Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AtariAgent:\n",
    "    \n",
    "    def __init__(self, game_id, num_actions=None, model_name=\"baseline_agent\"):\n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.game_id = game_id + \"NoFrameskip-v4\"\n",
    "        self.train_env = wrap_deepmind(gym.make(self.game_id))\n",
    "        self.test_env = wrap_deepmind(gym.make(self.game_id), clip_rewards=False)\n",
    "        \n",
    "        if num_actions is None:\n",
    "            self.num_actions = self.train_env.unwrapped.action_space.n\n",
    "        else:\n",
    "            self.num_actions = num_actions\n",
    "            \n",
    "        self.path = \"./\" + game_id + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = DistributionalQNetwork(self.num_actions, scope=\"agent\")\n",
    "        self.target_net = DistributionalQNetwork(self.num_actions, scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.1,\n",
    "                       annealing_steps=1000000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()   \n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end, info = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, r, s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    if self.train_env.unwrapped.ale.lives() == 0: break\n",
    "                    else: s = self.train_env.reset()\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              num_episodes=100000,\n",
    "              model_save_freq=500,\n",
    "              learning_curve_save_freq=100,\n",
    "              performance_print_freq=100,\n",
    "              model_test_freq=50000,\n",
    "              test_episodes_num=10):\n",
    "        \n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(self.init)\n",
    "            frame_count = 0\n",
    "            test_freq_threshold = 1\n",
    "            \n",
    "            train_rewards = []\n",
    "            test_rewards = []\n",
    "            frame_counts = []\n",
    "            \n",
    "            for i in range(num_episodes):\n",
    "                s = self.train_env.reset()\n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    if np.random.rand(1) < self.eps:\n",
    "                        a = np.random.randint(self.num_actions)\n",
    "                    else:\n",
    "                        a = self.agent_net.argmax_q(sess, [s])\n",
    "                    s_, r, end, info = self.train_env.step(a)\n",
    "                    self.rep_buffer.push(s, a, r, s_, end)\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                        \n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        max_actions = self.agent_net.argmax_q(sess, batch.s_)\n",
    "                        target_m = self.target_net.cat_proj(sess, batch.r, batch.s_, \n",
    "                                                            max_actions, batch.end)\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, target_m)\n",
    "                        \n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                            \n",
    "                    if end: \n",
    "                        if self.train_env.unwrapped.ale.lives() == 0: break\n",
    "                        else: s = self.train_env.reset()\n",
    "                \n",
    "                train_rewards.append(train_ep_reward)\n",
    "\n",
    "                if i % model_save_freq == 0:\n",
    "                    self.saver.save(sess, self.path+\"/model-\"+str(i)+\".ckpt\")\n",
    "                    print(\"Model saved.\")\n",
    "\n",
    "                # periodically save the learning curve\n",
    "                if i % learning_curve_save_freq == 0:\n",
    "                    np.savez(self.path+\"/learning_curve.npz\", r=train_rewards)\n",
    "\n",
    "                # periodically print model performance\n",
    "                if i % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, self.eps)\n",
    "\n",
    "                # periodicaly test greedy policy derived from the model\n",
    "                if frame_count // test_freq_threshold > 0:\n",
    "                    test_freq_threshold += model_test_freq\n",
    "                    one_test_rewards = []\n",
    "\n",
    "                    for j in range(test_episodes_num):\n",
    "                        test_ep_reward = 0\n",
    "                        s = self.test_env.reset()\n",
    "                        for time_step in range(self.max_ep_length):\n",
    "                            a = self.agent_net.argmax_q(sess, [s])\n",
    "                            s, r, end, info = self.test_env.step(a)\n",
    "                            test_ep_reward += r\n",
    "                            if end: \n",
    "                                if self.test_env.unwrapped.ale.lives() == 0: break\n",
    "                                else: s = self.test_env.reset()\n",
    "                        one_test_rewards.append(test_ep_reward)\n",
    "\n",
    "                    avg_reward = sum(one_test_rewards[-test_episodes_num:]) / test_episodes_num\n",
    "                    frame_counts.append(frame_count)\n",
    "                    test_rewards.append(one_test_rewards)\n",
    "\n",
    "                    print(\"Test info:\", frame_count, avg_reward)\n",
    "                    np.savez(self.path+'/test_curve.npz', frames=frame_counts, r=test_rewards)            \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-19 12:48:18,714] Making new env: BreakoutNoFrameskip-v4\n",
      "[2017-10-19 12:48:18,920] Making new env: BreakoutNoFrameskip-v4\n"
     ]
    }
   ],
   "source": [
    "aa = AtariAgent(\"Breakout\", model_name=\"baseline_distributional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=5000, replay_memory_size=100000, replay_start_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa.train(gpu_id=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
